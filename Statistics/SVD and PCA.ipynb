{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ae7ca1",
   "metadata": {},
   "source": [
    "# 1. 고유값과 고유벡터\n",
    "### (1) 정의\n",
    "$n\\times n$행렬 $A$에 대하여 $A\\vec{v}=\\lambda \\vec{v}$를 만족하는 $\\lambda$를 고유값, $\\vec{v}\\neq\\vec{0}$를 고유벡터라 한다.\n",
    "\n",
    "### (2) 도출\n",
    "$1.$ $Det(A-\\lambda I)=0$을 만족하는 고유값 $\\lambda$를 구한다.\n",
    "\n",
    "$2.$ 구한 고유값을 $(A-\\lambda I)\\vec{v}=0$에 대입하여 고유벡터 $\\vec{v}$를 구한다.\n",
    "\n",
    "$3.$ $n\\times n$행렬 $A$의 고유값 $\\lambda$와 고유벡터 $\\vec{v}$에 대하여 $E_\\lambda=Span(\\vec{v})$를 고유공간이라 한다.\n",
    "\n",
    "### (3) 정리\n",
    "$1.$ 삼각행렬의 고유값은 대각성분이다.\n",
    "\n",
    "$2.$ $n\\times n$행렬의 서로 다른 고유값을 $\\lambda_1, \\lambda_2, \\cdots, \\lambda_k$이라 하고 각각의 고유값에 해당하는 고유벡터를 $\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k$ 라고 하면 $\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$는 일차독립이다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf82267",
   "metadata": {},
   "source": [
    "# 2. Gram-Shmidt 직교화\n",
    "## 1) 두 벡터의 직교\n",
    "### (1) 정의\n",
    "내적공간 $V$의 원소 $\\vec{u},\\vec{v}$에 대하여 $\\vec{u}$와 $\\vec{v}$가 직교한다는 것은 내적이 $0$임을 의미한다. \n",
    "즉, $\\vec{u}\\perp \\vec{v} \\Leftrightarrow \\ <\\vec{u},\\vec{v}>=0.$\n",
    "\n",
    "## 2) 벡터의 크기(Norm)\n",
    "### (1) 정의\n",
    "내적공간 $V$의 원소 $\\vec{u}$에 대하여 벡터 $\\vec{u}$의 크기는 $||\\vec{u}||=\\sqrt{<\\vec{u},\\vec{u}>}$ 로 정의한다.\n",
    "\n",
    "### (2) 정리\n",
    "$1.\\ ||\\vec{u}+\\vec{v}||^2=||\\vec{u}||^2+||\\vec{v}||^2+2<\\vec{u},\\vec{v}>$\n",
    "\n",
    "$2.\\ ||\\vec{u}+\\vec{v}||^2+||\\vec{u}-\\vec{v}||^2=2||\\vec{u}||^2+2||\\vec{v}||^2$\n",
    "\n",
    "$3.\\ <\\vec{u},\\vec{v}>=\\dfrac{1}{4}||\\vec{u}+\\vec{v}||^2-\\dfrac{1}{4}||\\vec{u}-\\vec{v}||^2$\n",
    "\n",
    "$4.\\ \\vec{u}, \\vec{v}$ 가 직교할 필요충분조건은 $||\\vec{u}+\\vec{v}||^2=||\\vec{u}||^2+||\\vec{v}||^2$이다.\n",
    "\n",
    "$5.\\ |<\\vec{u}, \\vec{v}>| \\le||\\vec{u}|| \\ ||\\vec{v}||$ (단, 등호는 $\\vec{u}$와 $\\vec{v}$가 평행한 경우에 성립한다.)\n",
    "\n",
    "$6.\\ ||\\vec{u}+\\vec{v}|| \\le ||\\vec{u}|| + ||\\vec{v}||$\n",
    "\n",
    "> - $\\mathbb{R}^n$ 에서 정의된 닷곱 $\\vec{u}\\cdot \\vec{v}=\\vec{u}^T\\vec{v}$ 도 내적(Inner Product)이다.\n",
    "> - 따라서 $\\mathbb{R}^n$ 에서 정의된 닷곱에 대해서도 직교성을 논의할 수 있다. \n",
    "> - 이하의 내용은 일반적인 내적공간에 대해서도 성립하나 $\\mathbb{R}^n$ 에서 정의된 닷곱에 한정하여 서술한다.\n",
    "\n",
    "## 3) 직교집합(Orthogonal Set)\n",
    "### (1) 정의\n",
    "$\\vec{v}_1,\\ \\vec{v}_2,\\ \\cdots,\\ \\vec{v}_k \\in \\mathbb{R}^n$ 에서 정의된 닷곱에 대하여\n",
    "$\\{ \\vec{v}_1,\\ \\vec{v}_2,\\ \\cdots,\\ \\vec{v}_k \\}$가 직교집합이라는 것은 $\\vec{v}_i \\cdot \\vec{v}_j=0, \\quad i\\neq j$ 임을 뜻한다.\n",
    "\n",
    "### (2) 정리\n",
    "$1.$ 직교집합 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 에 의해 생성된 $\\mathbb{R}^n$의 부분공간 $V$ (즉, $V=Span(B) \\subseteq \\mathbb{R}^n$)의 임의의 벡터 $\\vec{u} \\in V$ 에 대하여,\n",
    "\n",
    "$\\quad \\vec{u}=\\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1 \\: + \\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2 \\: + \\: \\cdots \\: + \\dfrac{\\vec{u} \\cdot \\vec{v}_k}{\\vec{v}_k \\cdot \\vec{v}_k}\\vec{v}_k$ 로 나타낼 수 있다.\n",
    "\n",
    "$2.$ 직교집합 $\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 이 영벡터를 포함하지 않으면 $\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$는 일차독립이다.\n",
    "\n",
    "## 4) 직교기저(Orthogonal Basis)\n",
    "### (1) 정의\n",
    "$\\mathbb{R}^n$의 부분공간 $V$에 대하여 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 가 $V$의 ①기저이며 ②직교집합이면 $B$는 $V$의 직교기저이다.\n",
    "\n",
    "## 5) 정규직교집합(Orthonormal Set)\n",
    "### (1) 정의\n",
    "$\\{ \\vec{v}_1,\\ \\vec{v}_2,\\ \\cdots,\\ \\vec{v}_k \\}$가 정규직교집합이라는 것은\n",
    "① $<\\vec{v}_i, \\vec{v}_j>=0, \\quad i\\neq j$\n",
    "② $||\\vec{v}_i||=1, \\quad \\forall{i}$\n",
    "임을 뜻한다.\n",
    "\n",
    "> 참고: $\\mathbb{R}^n$의 표준기저는 정규직교집합이다.\n",
    "\n",
    "## 6) 정규직교기저(Orthonormal Basis)\n",
    "### (1) 정의\n",
    "$\\mathbb{R}^n$의 부분공간 $V$에 대하여 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 가 $V$의 ①기저이며 ②정규직교집합이면 $B$는 $W$의 직교기저이다.\n",
    "\n",
    "### (2) 정리\n",
    "$1.$ 정규직교집합 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 에 의해 생성된 $\\mathbb{R}^n$의 부분공간 $V$의 임의의 벡터 $\\vec{u} \\in V$ 에 대하여\n",
    "$\\quad \\vec{u} \\ = \\ (\\vec{u} \\cdot \\vec{v}_1)\\vec{v}_1 \\ + \\ (\\vec{u} \\cdot \\vec{v}_2)\\vec{v}_2 \\ + \\ \\cdots \\ + \\ (\\vec{u} \\cdot \\vec{v}_k)\\vec{v}_k$ 로 나타낼 수 있다.\n",
    "\n",
    "$2.$ $m\\times n \\ (m\\ge n)$ 행렬 $A$의 열벡터들의 정규직교이면 $A^TA=I_n$이다.\n",
    "\n",
    "## 7) 직교행렬(Orthogonal Matrix)\n",
    "### (1) 정의\n",
    "$A$가 직교행렬이라는 것은 ① $A: n \\times n$ 행렬, ② $A$의 열벡터들이 **정규직교** 집합일 때이다.\n",
    "\n",
    "### (2) 정리\n",
    "$1.$ 정사각행렬 $A$가 직교행렬일 필요충분조건은 $A^TA=I$ 즉, $A^T=A^{-1}$이다.       \n",
    "$2.$ $A^T=A^{-1}$ 으로부터 $AA^T=I$ 이기도 한데, 이는 정사각행렬 $A$가 직교행렬이라면 $A$의 행벡터들의 집합도 정규직교집합임을 의미한다.                      \n",
    "\n",
    "## 8) 직교사영(Orthogonal Projection)\n",
    "### (1) 도출\n",
    "#### [1] $\\mathbb{R}^2$에서의 직교사영\n",
    "\n",
    "일차독립인 $\\vec{u},\\vec{v} \\in \\mathbb{R}^2$ 에 대하여 (즉, $\\vec{u},\\vec{v}$는 $\\mathbb{R}^2$의 기저)\n",
    "\n",
    "① $\\vec{u_{pr}}=\\dfrac{\\vec{u} \\cdot \\vec{v}}{\\vec{v} \\cdot \\vec{v}}\\vec{v}\\qquad$: $\\vec{v}$ 위로 $\\vec{u}$ 의 직교사영 (벡터사영)\n",
    "\n",
    "② $\\vec{u_c}=\\vec{u}-\\vec{u_{pr}} \\qquad$ : $\\vec{v}$ 에 직교하는 $\\vec{u}$의 벡터성분\n",
    "\n",
    "(i) $\\vec{u_{pr}}+\\vec{u_c}=\\vec{u} \\qquad$\t(ii) $\\vec{u_{pr}} \\parallel \\vec{v} \\qquad$ (iii) $\\vec{u_c} \\perp \\vec{v}$\n",
    "\n",
    "\n",
    "<center><img src='image/SVD_orthogonal_projection_r2.jpg', width = 600></center>\n",
    "\n",
    "\n",
    "#### [2] $\\mathbb{R}^3$에서의 직교사영\n",
    "일차독립인 $\\vec{u},\\vec{v}_1,\\vec{v}_2  \\in \\mathbb{R}^3$, 직교하는 벡터 $\\vec{v}_1,\\vec{v}_2$ 에 대하여 \n",
    "\n",
    "① $\\vec{u_{1}}=\\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1\\qquad$: $\\vec{v}_1$ 위로 $\\vec{u}$ 의 직교사영\n",
    "\n",
    "② $\\vec{u_{2}}=\\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2\\qquad$: $\\vec{v}_2$ 위로 $\\vec{u}$ 의 직교사영\n",
    "\n",
    "③ $\\vec{u_{pr}}= \\vec{u_{1}} + \\vec{u_{2}} = \\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1 + \\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2 \\qquad$ : $Span(\\vec{v}_1,\\vec{v}_2)$ 위로 $\\vec{u}$ 의 직교사영\n",
    "\n",
    "④ $\\vec{u_c}=\\vec{u}-\\vec{u_{pr}}=\\vec{u}-\\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1 - \\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2 \\qquad$ : $Span(\\vec{v}_1,\\vec{v}_2)$ 에 직교하는 $\\vec{u}$ 의 벡터성분\n",
    "\n",
    "\n",
    "<center><img src='image/SVD_orthogonal_projection_r3.jpg', width = 600></center>\n",
    "\n",
    "\n",
    "#### [3] $\\mathbb{R}^n$에서의 직교사영\n",
    "\n",
    "$\\mathbb{R}^n$의 부분공간 $V$에 대한 직교기저 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 가 주어져 있을 때, $V$의 임의의 벡터 $\\vec{u}$ 에 대하여 \n",
    "\n",
    "① $\\vec{u_{pr}}=\\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1 \\: + \\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2 \\: + \\: \\cdots \\: + \\dfrac{\\vec{u} \\cdot \\vec{v}_k}{\\vec{v}_k \\cdot \\vec{v}_k}\\vec{v}_k \\qquad$ : $V$위로의 $\\vec{u}$의 직교사영\n",
    "\n",
    "② $\\vec{u_c}=\\vec{u}-\\vec{u_{pr}}=\\vec{u}-\\dfrac{\\vec{u} \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1}\\vec{v}_1 \\: - \\dfrac{\\vec{u} \\cdot \\vec{v}_2}{\\vec{v}_2 \\cdot \\vec{v}_2}\\vec{v}_2 \\: - \\: \\cdots \\: - \\dfrac{\\vec{u} \\cdot \\vec{v}_k}{\\vec{v}_k \\cdot \\vec{v}_k}\\vec{v}_k \\qquad$ : $V$에 직교하는 $\\vec{u}$의 벡터성분\n",
    "\n",
    "\n",
    "\n",
    "### (2) 정리: Gram-Schmidt 직교화과정\n",
    "$\\mathbb{R}^n$의 부분공간 $V$에 대하여 $B=\\{ \\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_k \\}$ 가 $V$의 기저일 때, 직교기저 $C=\\{ \\vec{u}_1, \\vec{u}_2, \\cdots, \\vec{u}_k \\}$ 와 \n",
    "정규직교기저 $D=\\{ \\vec{w}_1, \\vec{w}_2, \\cdots, \\vec{w}_k \\}$도 존재한다. 여기서 $C$와 $D$는 다음과 같이 구할 수 있다. \n",
    "\n",
    "$\\vec{u}_1=\\vec{v}_1$\n",
    "\n",
    "$\\vec{u}_2=\\vec{v}_2-\\dfrac{\\vec{v}_2 \\cdot \\vec{u}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1$\n",
    "\n",
    "$\\vec{u}_3=\\vec{v}_3-\\dfrac{\\vec{v}_3 \\cdot \\vec{v}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1-\\dfrac{\\vec{v}_3 \\cdot \\vec{u}_2}{\\vec{u}_2 \\cdot \\vec{u}_2}\\vec{u}_2$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$\\vec{u}_k=\\vec{v}_k-\\dfrac{\\vec{v}_k \\cdot \\vec{u}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1-\\dfrac{\\vec{v}_k \\cdot \\vec{u}_2}{\\vec{u}_2 \\cdot \\vec{u}_2}\\vec{u}_2 - \\: \\cdots \\: -\\dfrac{\\vec{v}_k \\cdot \\vec{u}_{k-1}}{\\vec{u}_{k-1} \\cdot \\vec{u}_{k-1}}\\vec{u}_{k-1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\vec{w}_1=\\dfrac{\\vec{u}}{||u_1||}, \\quad \\vec{w}_2=\\dfrac{\\vec{u}}{||u_2||}, \\quad \\cdots, \\quad \\vec{w}_k=\\dfrac{\\vec{u}}{||u_k||}$\n",
    "\n",
    "> ① $\\vec{u}_1=\\vec{v}_1\\qquad$이제 $\\vec{u}_1$를 만들었으니 $\\vec{v}_1$는 사용하지 않는다.\n",
    "> ② $\\vec{u}_1$ 와 직교하는 벡터 $\\vec{u}_2$ 를 찾고 싶은 것이므로 \n",
    ">\n",
    "> $\\quad \\vec{u}_2=\\vec{v}_{2c}=\\vec{v}_2-\\vec{v}_{2pr}=\\vec{v}_2-\\dfrac{\\vec{v}_2 \\cdot \\vec{u}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1$\n",
    ">\n",
    "> <center><img src='image/SVD_gram_schmidt_r2.jpg', width = 600></center>\n",
    ">\n",
    "> $\\quad$ 이제 $\\vec{u}_2$를 만들었으니 $\\vec{v}_2$는 사용하지 않는다.\n",
    "> <br>\n",
    "> ③ 위에서 $\\vec{u}_1$과 $\\vec{u}_2$는 직교하게 만들었으므로, $Span(\\vec{u}_1,\\vec{u}_2)$과 직교하는 $\\vec{u}_3$을 찾으려면  \n",
    "> \n",
    "> \n",
    "> $\\quad \\vec{u}_2=\\vec{v}_{3c}=\\vec{v}_3-\\vec{v}_{3pr}=\\vec{v}_3-\\dfrac{\\vec{v}_3 \\cdot \\vec{u}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1-\\dfrac{\\vec{v}_3 \\cdot \\vec{u}_2}{\\vec{u}_2 \\cdot \\vec{u}_2}\\vec{u}_2$\n",
    ">\n",
    "> <center><img src='image/SVD_gram_schmidt_r3.jpg', width = 600></center>\n",
    ">\n",
    "> $\\quad$ 이제 $\\vec{u}_3$를 만들었으니 $\\vec{v}_3$는 사용하지 않는다.\n",
    "> <br>\n",
    "> ④ 이를 반복하면,\n",
    ">\n",
    "> $\\quad \\vec{u}_k=\\vec{v}_k-\\dfrac{\\vec{v}_k \\cdot \\vec{u}_1}{\\vec{u}_1 \\cdot \\vec{u}_1}\\vec{u}_1-\\dfrac{\\vec{v}_k \\cdot \\vec{u}_2}{\\vec{u}_2 \\cdot \\vec{u}_2}\\vec{u}_2 - \\: \\cdots \\: -\\dfrac{\\vec{v}_k \\cdot \\vec{u}_{k-1}}{\\vec{u}_{k-1} \\cdot \\vec{u}_{k-1}}\\vec{u}_{k-1}$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c8ade",
   "metadata": {},
   "source": [
    "# 3. 특이값 분해(SVD; Singular Value Decomposition)\n",
    "\n",
    "### (1) 정의\n",
    "$m\\times n$행렬 $A$에 대하여 $A=U\\Sigma V^T$ 의 형태로 표현할 수 있다. 여기서\n",
    "$V: n\\times n$ 직교행렬, $\\quad \\Sigma: m\\times n$ 행렬이다. 특히\n",
    "\n",
    "$$\n",
    "\\Sigma=\\begin{bmatrix} D & 0 \\\\ 0 & 0 \\end{bmatrix} \n",
    ", D=\\begin{bmatrix} \\sigma_1 & 0 & \\cdots & 0 \\\\ 0 & \\sigma_2 &\\cdots & 0 \\\\ \\cdots & \\cdots & \\cdots &\\cdots \\\\ 0 & 0 & \\cdots & \\sigma_r\\end{bmatrix} \n",
    "$$ \n",
    "> <center><img src='image/SVD_SVD.webp', width = 900></center>\n",
    ">\n",
    "> <div align ='center'>\n",
    ">\n",
    "> [Singular Value Decomposition (SVD), Demystified](https://towardsdatascience.com/singular-value-decomposition-svd-demystified-57fc44b802a0/)\n",
    ">\n",
    "> </div>\n",
    "\n",
    "### (2) 도출\n",
    "① 대칭행렬 $A^TA$를 구한다. (직교대각화 가능)\n",
    "\n",
    "② $A^TA$의 고유값 $\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$을 구한다.\n",
    "\n",
    "③ 고유값 $\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$에 해당하는 고유벡터를 구한 뒤, 이를 **정규직교** 고유벡터 $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$로 바꿔준다.\n",
    "- 이 때 서로 다른 고유값으로부터 나온 고유벡터들은 서로 직교성이 보장되어 있지만 중근이 존재하여 하나의 고유값으로부터 나온 여러개의 고유벡터가 있다면 그들은 서로 직교성이 보장되어 있지 않으므로 Gram-Shmidt 직교화과정을 통해 직교성을 갖게끔 바꿔주어야 한다. \n",
    "\n",
    "④ 특이값 $\\quad \\sigma_1=\\sqrt{\\lambda_1} \\quad \\ge \\quad \\sigma_2=\\sqrt{\\lambda_2} \\quad \\ge \\quad \\cdots \\quad \\ge \\quad  \\sigma_n=\\sqrt{\\lambda_n} \\quad \\ge \\quad 0$ 을 구한다.\n",
    "$\\quad \\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0, \\quad \\sigma_{r+1}=\\sigma_{r+2}=\\cdots=\\sigma_n=0$ 이라 하자.\n",
    "\n",
    "⑤ $\\vec{u}_i=\\dfrac{1}{\\sigma_i}A\\vec{v}_i \\quad (i=1,2,\\cdots,r)$ 을 구한다.\n",
    "\n",
    "⑥ 정규직교집합 $\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_r\\} \\subset{\\mathbb{R}^n}$ 으로부터 $\\mathbb{R}^m$ 의 정규직교기저 $\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_r,\\vec{u}_{r+1},\\cdots,\\vec{u}_m\\}$ 을 구한다.\n",
    "- $\\mathbb{R}^n$의 표준기저들 중 정규직교집합 $\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_r\\}$과 일차독립인 표준기저들을 찾아 Gram-shmidt 직교화과정을 통해 구할 수 있다.\n",
    "\n",
    "⑦ $U=[\\vec{u}_1 \\quad \\vec{u}_2 \\quad \\cdots \\quad \\vec{u}_m], \\qquad V=[\\vec{v}_1 \\quad \\vec{v}_2 \\quad \\cdots \\quad \\vec{v}_n]$\n",
    "\n",
    "### (3) 정리\n",
    "$m\\times n$행렬 $A$에 대하여 $V, \\Sigma, U$ 를 특이값분해행렬이라 하고, $\\sigma_1, \\sigma_2, \\cdots, \\sigma_r$  을 영이 아닌 $A$의 특이값 전체라고 하면\n",
    "\n",
    "$$\n",
    "V=\\begin{bmatrix} \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_r & | &\\vec{v}_{r+1} & \\vec{v}_{r+2} & \\cdots & \\vec{v}_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix} \\vec{u}_1 & \\vec{u}_2 & \\cdots & \\vec{u}_r & | &\\vec{u}_{r+1} & \\vec{u}_{r+2} & \\cdots & \\vec{u}_m \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "\n",
    "(i) $\\quad rank(A)=r$       \n",
    "(ii) $\\:\\:\\:\\: \\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_r\\}\\quad: Col(A)$의 정규직교기저        \n",
    "(iii) $\\:\\:\\: \\{\\vec{u}_{r+1},\\vec{u}_{r+2},\\cdots,\\vec{u}_m\\}\\quad: Null(A^T)$의 정규직교기저      \n",
    "(iv) $\\:\\:\\: \\{\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_r\\}\\quad: Row(A)$의 정규직교기저      \n",
    "(v) $\\quad \\{\\vec{v}_{r+1},\\vec{v}_{r+2},\\cdots,\\vec{v}_n\\}\\quad: Null(A)$의 정규직교기저       \n",
    "\n",
    "### (4) 유사역원(Pseudo Inverse, Moore-Penrose Inverse)\n",
    "$m\\times n$행렬 $A$에 대하여 $V, \\Sigma, U$ 를 특이값분해행렬이라 하자. 즉, $A=U\\Sigma V^T$ 이다. \n",
    "이 때  $A^+=U\\Sigma^+ V^T$ 를 $A$의 유사역원이라 한다. 여기서 $\\Sigma^+$는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\Sigma^+=\\begin{bmatrix} D^{-1} & 0 \\\\ 0 & 0 \\end{bmatrix}_{n\\times m}, \\qquad\n",
    "\n",
    "D^{-1}=\\begin{bmatrix} \\sigma_1^{-1} & 0 & \\cdots & 0 \\\\ 0 & \\sigma_2^{-1} &\\cdots & 0 \\\\ \\cdots & \\cdots & \\cdots &\\cdots \\\\ 0 & 0 & \\cdots & \\sigma_r^{-1}\\end{bmatrix}_{r\\times r}\n",
    "$$ \n",
    "\n",
    "만약 $A$가 가역행렬이면 $A^{-1}=A^+$ 이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "_**출처: 학부 및 대학원에서 본인의 Lecture Note - 선형대수학**_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d7ce3",
   "metadata": {},
   "source": [
    "# 4. 주성분 분석(PCA; Principal Component Analysis)\n",
    "## (1) 목적\n",
    "* 고차원 데이터를 정보 손실을 최소화하면서 저차원으로 투영하는 것이다.  \n",
    "* 분산이 클수록 데이터를 잘 설명할 수 있다.\n",
    "* 따라서, 데이터의 분산이 가장 큰 방향(주성분)을 찾는 것이 핵심이다.\n",
    "\n",
    "## (2) 방법\n",
    "기존 입력 변수를 표현하는 좌표축들을 다른 좌표축으로 변환시킨 뒤 데이터를 가장 잘 설명할 수 있는(분산이 가장 큰) 몇몇 차원만 선택하여 차원을 줄인다.\n",
    "\n",
    "> **예시**\n",
    ">\n",
    "> **Step 1.** 2개의 feature X, Y 존재 <br>\n",
    ">\n",
    "> <img src='image/PCA_pca1.webp' width=\"600\" style=\"display: block; margin: 0;\"><br>\n",
    "> **Step 2.** 분산이 가장 큰 좌표축과 해당 좌표축과 서로 직교하는 새로운 축을 찾는다 → V1 (분산이 가장 큰 좌표축), V2 <br>\n",
    ">\n",
    "> <img src='image/PCA_pca2.webp' width=\"600\" style=\"display: block; margin: 0;\"><br>\n",
    "> **Step 3.** V1, V2를 새로운 좌표축 PC1, PC2로 만들고, 데이터를 가장 잘 설명할 수 있는 좌표축 PC1만 남겨둔다. <br>\n",
    ">\n",
    "> <img src='image/PCA_pca3.webp' width=\"600\" style=\"display: block; margin: 0;\">\n",
    ">\n",
    "> [Principal Component Analysis Guide & Example](https://statisticsbyjim.com/basics/principal-component-analysis/)\n",
    "\n",
    "## (2) 수식을 이용한 구체적인 단계 설명\n",
    "### [1] 데이터 정규화 (평균 제거)\n",
    "$X$ 를 $n$개의 관측치와 $p$개의 feature를 가지고 있는 데이터라고 하자: $X \\in \\mathbb{R}^{n\\times p}$     \n",
    "우선 각 feature에 대해 중심화를 수행하여 각 특성의 평균이 0이 되게 한다.   \n",
    "각 관측치 $i=1,2,\\cdots,n$ 와 feature $j=1,2,\\cdots,p$ 에 대해\n",
    "\n",
    "① 열별 평균 계산\n",
    "\n",
    "$\\mu_j = \\dfrac{1}{n} \\sum_{i=1}^n X_{ij}, \\quad \\text{for } j = 1, 2, \\dots, p$\n",
    "\n",
    "이를 벡터로 표시하면, \n",
    "\n",
    "$\\vec{\\mu} = \\dfrac{1}{n} X^\\top \\mathbf{1}_n$, $\\quad \\mathbf{1}_n \\in \\mathbb{R}^{n\\times 1}$ 은 모든 성분이 1인 열벡터.\n",
    "\n",
    "따라서 $\\vec{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{bmatrix} \\in \\mathbb{R}^{p\\times 1}$ 인 p차원의 열벡터이다.\n",
    "\n",
    "② 평균 제거\n",
    "\n",
    "$\\tilde{X}_{ij} = X_{ij} - \\mu_j, \\quad \\text{for } i = 1, \\dots, n,\\ j = 1, \\dots, p$\n",
    "\n",
    "이를 벡터로 표시하면,\n",
    "\n",
    "$\\tilde{X} = X - \\mathbf{1}_n \\vec{\\mu}^\\top ,\\quad \\tilde{X} \\in \\mathbb{R}^{n\\times p}$\n",
    "\n",
    "\n",
    "### [2] SVD 수행\n",
    "\n",
    "$\\tilde{X} = U \\Sigma V^\\top$\n",
    "\n",
    "이 때       \n",
    "① $V$는 직교행렬이므로 $V$의 열벡터들은 정규직교이며,         \n",
    "② 각 특이값(표준편차) \n",
    "$\\sigma_1=\\sqrt{\\lambda_1} \\ge \\sigma_2=\\sqrt{\\lambda_2} \\ge \\cdots \\ge \\sigma_n=\\sqrt{\\lambda_n} \\ge 0$ 으로부터 도출된 정규직교 벡터들을 대응되게 순서대로 나열한 행렬 $ V=[\\vec{v}_1 \\quad \\vec{v}_2 \\quad \\cdots \\quad \\vec{v}_n]$ 이다.        \n",
    "$\\quad$ **즉, $V$의 열벡터들은 특이값의 제곱(=분산)의 내림차순 기준으로 정렬되어 있다.**\n",
    "\n",
    "> **공분산 행렬과 SVD**       \n",
    "> * 평균 0으로 중심화된 데이터에 대해 행렬 $\\tilde{X} \\in \\mathbb{R}^{n\\times p}$ 의 공분산 행렬은 \n",
    "> $Cov(\\tilde{X}) = \\frac{1}{n} \\tilde{X}^\\top \\tilde{X}$ 인데 이 공분산행렬을 고유값 분해하면          \n",
    "> $Cov(\\tilde{X}) = V \\Lambda V^\\top$ ($\\Lambda = diag(\\lambda_1,\\cdots,\\lambda_p)$ : 고유값들(분산 크기, 내림차순), $V \\in \\mathbb{R}^{p\\times p}$ : 각 $\\lambda$ 에 대응되는 고유벡터들)이다.      \n",
    "> \n",
    "> * 이제 $\\tilde{X}$ 를 SVD 분해한 $\\tilde{X} = U \\Sigma V^\\top$ 을 대입하면 $U=[\\vec{u}_1 \\quad \\vec{u}_2 \\quad \\cdots \\quad \\vec{u}_n]$ 은 직교행렬 $(\\{\\vec{u}_1, \\cdots,\\vec{u}_n\\}$ 가 정규직교집합$)$ 이므로         \n",
    "> $$\n",
    "> \\tilde{X}^\\top \\tilde{X}\n",
    "> = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top)\n",
    "> = V \\Sigma^\\top U^\\top U \\Sigma V^\\top\n",
    "> = V \\Sigma^2 V^\\top\n",
    "> $$\n",
    ">         \n",
    "> $\\qquad \\therefore \\tilde{X}^\\top \\tilde{X}$ 의 고유값: $\\Sigma^2$ 의 대각 원소 (**즉, 분산**)          \n",
    "> $\\qquad \\quad \\tilde{X}^\\top \\tilde{X}$ 의 고유벡터: $V$ 의 열벡터\n",
    "\n",
    "\n",
    "### [3] 주성분 선택\n",
    "\n",
    "$V=[\\vec{v}_1 \\quad \\vec{v}_2 \\quad \\cdots \\quad \\vec{v}_n] \\in \\mathbb{R}^{p \\times p}$            \n",
    "상위 $k$개의 주성분 선택: \n",
    "$W = [\\vec{v}_1 \\quad \\vec{v}_2 \\quad \\cdots \\quad \\vec{v}_k] \\in \\mathbb{R}^{p \\times k}$\n",
    "\n",
    "\n",
    "### [4] 데이터 차원 축소\n",
    "\n",
    "$Z = \\tilde{X} W \\in \\mathbb{R}^{n \\times k}$\n",
    "→ 이것이 원래 데이터 $X$를 $k$-차원 주성분 공간에 투영한 결과이다.\n",
    "\n",
    "\n",
    "### [5] 원래 차원으로 복원 (선택)\n",
    "\n",
    "$\\hat{X} = Z W^\\top + \\bar{X} \\qquad \\text{where} \\quad \\bar{X} = \\mathbf{1}_n \\vec{\\mu}^\\top \\in \\mathbb{R}^{n\\times p}$ (데이터의 열평균 벡터, 즉 중심화 이전의 평균값)         \n",
    "→ 복원된 데이터 $\\hat{X} \\in \\mathbb{R}^{n \\times p}$\n",
    "\n",
    "<br>\n",
    "\n",
    "> **참고: [1] 데이터 정규화에서 분산까지 1로 만드는 경우 (표준화: 평균 0, 분산 1)**         \n",
    "> PCA에 반드시 필요한 과정은 아니지만, 각 feature의 단위(scale)이 매우 다를 때 수행한다:        \n",
    "> $\\qquad \\qquad \\qquad \\qquad \\text{standardized } \\tilde{X} = \\dfrac{X_{ij}-\\mu_j}{\\sigma_j}$         \n",
    "> 벡터로 표현하는 경우: $X^{std} = (X - \\vec{\\mu}) \\oslash \\vec{\\sigma}$          \n",
    "> $\\qquad \\qquad \\qquad \\qquad \\vec{\\mu} \\in \\mathbb{R}^{1 \\times p} : $ 평균 벡터              \n",
    "> $\\qquad \\qquad \\qquad \\qquad \\vec{\\sigma} \\in \\mathbb{R}^{1 \\times p} : $ 표준편차 벡터               \n",
    "> $\\qquad \\qquad \\qquad \\qquad \\oslash : $ 열 단위로 나누는 브로드캐스팅 연산 (element-wise division)             \n",
    "> - 표준화하는 경우 이제 [2]에서 **공분산 행렬이 아니라 상관계수 행렬(correlation matrix)**의 고유분해를 수행하는 것과 같다.          \n",
    "> - 결과적으로 찾는 주성분 방향은 동일한 방식으로 구해지지만, 단위가 정규화된 축에서의 최대 분산 방향이 될 뿐이다.           \n",
    "\n",
    "\n",
    "## (3) 선형변환으로서의 PCA\n",
    "\n",
    "PCA의 투영:  $Z = \\tilde{X} W \\qquad W = [\\vec{v}_1, \\dots, \\vec{v}_k] \\in \\mathbb{R}^{p\\times k}, \\quad Z \\in \\mathbb{R}^{n\\times k}$    \n",
    "$\\Rightarrow$ **따라서 PCA는 원점을 지나면서 기저를 회전 및 축소시키는 선형사상(linear transformation)이다.**\n",
    "\n",
    "> $\\tilde{X}$ 의 $i$번째 행벡터(샘플)의 전치를 $\\tilde{x}_i$ 라고 하자. ($\\vec{\\tilde{x}_i} \\in \\mathbb{R}^{p}$ 는 $p$차원 열벡터)      \n",
    "> 또한 $\\vec{z}_i$ 를 $Z$ 의 $i$번째 행벡터(샘플)의 전치라 하면,          \n",
    "> 각 샘플 $\\vec{\\tilde{x}_i}$ 에 대해 $\\vec{z}_i = W^\\top\\vec{\\tilde{x}_i} \\in \\mathbb{R}^{k}$ ($k$차원 열벡터) 이고,           \n",
    "> 이는 곧 PCA가 $\\vec{\\tilde{x}_i}$ 를 $W^\\top$ 라는 행렬을 곱해서 저차원 벡터 $\\vec{z}_i$ 로 보내는 변환을 의미한다.         \n",
    "\n",
    "<br>\n",
    "\n",
    "_**출처: 대학원에서 본인의 Lecture Note - Linear Regression, Macroeconometrics, Advanced Machine Learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00b80e",
   "metadata": {},
   "source": [
    "# 5. Truncated SVD\n",
    "\n",
    "기존 SVD를 다시 써 보면 다음과 같다.\n",
    "\n",
    "**SVD:**\n",
    "\n",
    "$$\n",
    "A=U\\Sigma V^T\n",
    "$$ \n",
    "\n",
    "- $A \\in \\mathbb{R}^{m\\times n}$\n",
    "- $U \\in \\mathbb{R}^{n\\times n}$\n",
    "- $\\Sigma \\in \\mathbb{R}^{n\\times p}$\n",
    "- $V \\in \\mathbb{R}^{p\\times p}$\n",
    "\n",
    "\n",
    "한편 PCA에서는 $V=[\\vec{v}_1 \\quad \\vec{v}_2 \\quad \\cdots \\quad \\vec{v}_n] \\in \\mathbb{R}^{p \\times p}$ 중 상위 $k$개의 주성분만 선택하는데, 이에 대응되는 상위 $k$ 개의 특이값(=정보량이 큰 주성분)을 $\\Sigma$ 에서 선택하고, 또 $U$ 에서 대응되는 앞쪽의 $k$ 개의 열벡터만 선택하여 SVD를 다음과 같이 다시 써 볼 수 있다. 이를 Truncated SVD라 한다.\n",
    "\n",
    "**Truncated SVD:**\n",
    "\n",
    "$$\n",
    "A\\approx U_k\\Sigma_k V_{k}^T\n",
    "$$ \n",
    "\n",
    "- $A \\in \\mathbb{R}^{m\\times n}$\n",
    "- $U_k \\in \\mathbb{R}^{m\\times k}$ : 앞쪽에서부터 대응되는 $k$ 개의 열벡터만 남긴 행렬\n",
    "- $\\Sigma_k \\in \\mathbb{R}^{k\\times k}$ : 상위 $k$ 개의 특이값만 남긴 대각행렬\n",
    "- $V_k \\in \\mathbb{R}^{n\\times k}$ : 앞쪽에서부터 대응되는 $k$ 개의 열벡터만 남긴 행렬\n",
    "  \n",
    "\n",
    "**따라서 PCA를 위한 SVD를 할 것이라면 처음부터 Truncated SVD를 구해도 된다.**\n",
    "\n",
    "$$\n",
    "\\tilde{X} \\approx U_k \\Sigma_k V_{k}^\\top \\qquad \\Rightarrow \\qquad Z = \\tilde{X} W = \\tilde{X} V_k (= U_k \\Sigma_k \\,\\,\\, V_{k}^\\top V_k) = U_k \\Sigma_K\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
